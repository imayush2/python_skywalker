{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "download_dir_for_xml_files = r\"C:\\Users\\abhishek.i.bhardwaj\\Desktop\\Python\\who\\Application\\XML_Files\" # Enter Folder path where all the xml files will be downloaded\n",
    "download_dir_for_excel_files = r\"C:\\Users\\abhishek.i.bhardwaj\\Desktop\\Python\\who\\Application\\Database\" # Enter Folder path where all EXCEL files will be downloaded\n",
    "\n",
    "title = \"Liver Transplant\" # Enter the Disease to search\n",
    "countries = [\"Germany\"] # Enter Countries\n",
    "whotrialsearch = \"https://trialsearch.who.int/\"\n",
    "pubmedsearch = \"https://pubmed.ncbi.nlm.nih.gov/\" \n",
    "google = \"https://www.google.com/\"\n",
    "\n",
    "def initialize_driver():\n",
    "    chrome_options = Options()\n",
    "    prefs = {\n",
    "        \"download.default_directory\": download_dir_for_xml_files,  # Custom download folder\n",
    "        \"download.prompt_for_download\": False,        # Don't prompt, just download\n",
    "        \"download.directory_upgrade\": True,           # Overwrite files without asking\n",
    "        \"safebrowsing.enabled\": True                  # Enable safe browsing\n",
    "    }\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "    # chrome_options.add_argument('--disable-gpu')  # Disable GPU hardware acceleration (optional)\n",
    "    # chrome_options.add_argument('--window-size=1920x1080')  # Set the window size to avoid issues with some websites\n",
    "    # chrome_options.add_argument('--no-sandbox') \n",
    "    # chrome_options.add_argument('--disable-software-rasterizer')\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    return driver\n",
    "\n",
    "initial_count = None\n",
    "\n",
    "def check_for_new_files(download_dir, file_extension=\".xml\", timeout=30, initial_count=None, wait_time=5):\n",
    "    \n",
    "    if initial_count is None:\n",
    "        initial_count = len([f for f in os.listdir(download_dir) if f.endswith(file_extension)])\n",
    "\n",
    "    # Wait for the files to download (for example, wait 5 seconds after triggering download)\n",
    "    time.sleep(wait_time)\n",
    "    \n",
    "    # Get the current count of files\n",
    "    current_count = len([f for f in os.listdir(download_dir) if f.endswith(file_extension)])\n",
    "\n",
    "    # Check if the file count has increased\n",
    "    if current_count > initial_count:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_latest_file(download_dir, file_extension=\".xml\"):\n",
    "    # List all files with the specified extension in the directory\n",
    "    files = [f for f in os.listdir(download_dir) if f.endswith(file_extension)]\n",
    "    \n",
    "    if not files:  # If no files found, return None\n",
    "        return None\n",
    "    \n",
    "    # Get the full path for each file and its last modification time\n",
    "    files_with_times = [(f, os.path.getmtime(os.path.join(download_dir, f))) for f in files]\n",
    "    \n",
    "    # Sort files by modification time in descending order (latest file first)\n",
    "    latest_file = max(files_with_times, key=lambda x: x[1])\n",
    "    \n",
    "    return latest_file[0]  \n",
    "\n",
    "def run_scraping_who(title, countries):\n",
    "    driver = initialize_driver()\n",
    "    driver.get(whotrialsearch)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Go to the Advanced Search Page\n",
    "    advance_search = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"Menu1n1\"]/table/tbody/tr/td/a')))\n",
    "    advance_search.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter search item\n",
    "    search_input_title = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_txtTitle\"]')))\n",
    "    search_input_title.send_keys(title)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Add countries\n",
    "    if len(countries)>0:\n",
    "        for country in countries:\n",
    "            country_search = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_txtFreeCountry\"]')))\n",
    "            country_search.send_keys(country)\n",
    "            time.sleep(1)\n",
    "            country_submit = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_butAdd\"]')))\n",
    "            country_submit.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "    search_btn = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_btnSearch\"]')))\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Click to download XML data\n",
    "    xml_btn = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_btnLaunchDialogTerms\"]')))\n",
    "    xml_btn.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    agree_btn = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_btnExport\"]')))\n",
    "    agree_btn.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    export_all_trials = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_ucExportDefault_butExportAllTrials\"]')))\n",
    "    export_all_trials.click()\n",
    "    time.sleep(4)\n",
    "\n",
    "    if check_for_new_files(download_dir_for_xml_files, file_extension='xml', timeout=30, initial_count=initial_count, wait_time=5):\n",
    "        driver.close()\n",
    "    else:\n",
    "        time.sleep(30)\n",
    "        if check_for_new_files(download_dir_for_xml_files, file_extension='xml', timeout=30, initial_count=initial_count, wait_time=5):\n",
    "            driver.close()\n",
    "\n",
    "    return title\n",
    "\n",
    "def read_latest_xml_file(download_dir):\n",
    "\n",
    "    data = []\n",
    "    # Get the latest file in the directory\n",
    "    latest_file = get_latest_file(download_dir, file_extension=\".xml\")\n",
    "    \n",
    "    if not latest_file:  # If no file is found, return a message\n",
    "        print(\"No XML file found.\")\n",
    "        return\n",
    "    \n",
    "    # Get the full path of the latest file\n",
    "    latest_file_path = os.path.join(download_dir, latest_file)\n",
    "\n",
    "    tree = ET.parse(latest_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for elem in root.findall('.//'):\n",
    "        # Extract data\n",
    "        export_date = elem.find('Export_date').text if elem.find('Export_date') is not None else ''\n",
    "        trial_id = elem.find('TrialID').text if elem.find('TrialID') is not None else ''\n",
    "        last_refreshed = elem.find('Last_Refreshed_on').text if elem.find('Last_Refreshed_on') is not None else ''\n",
    "        public_title = elem.find('Public_title').text if elem.find('Public_title') is not None else ''\n",
    "        primary_sponsor = elem.find('Primary_sponsor').text if elem.find('Primary_sponsor') is not None else ''\n",
    "        web_address = elem.find('web_address').text if elem.find('web_address') is not None else ''\n",
    "        recruitment_status = elem.find('Recruitment_Status').text if elem.find('Recruitment_Status') is not None else ''\n",
    "        countries = elem.find('Countries').text if elem.find('Countries') is not None else ''\n",
    "        contact_firstname = elem.find('Contact_Firstname').text if elem.find('Contact_Firstname') is not None else ''\n",
    "        contact_lastname = elem.find('Contact_Lastname').text if elem.find('Contact_Lastname') is not None else ''\n",
    "        contact_address = elem.find('Contact_Address').text if elem.find('Contact_Address') is not None else ''\n",
    "        contact_email = elem.find('Contact_Email').text if elem.find('Contact_Email') is not None else ''\n",
    "        contact_tel = elem.find('Contact_Tel').text if elem.find('Contact_Tel') is not None else ''\n",
    "        Contact_Affiliation = elem.find('Contact_Affiliation').text if elem.find('Contact_Affiliation') is not None else ''\n",
    "        # Append data to the list\n",
    "        \n",
    "        data.append({\n",
    "            'Source': 'WHO International Trial Registry Platform',\n",
    "            'Data Export Date': export_date,\n",
    "            'Trial ID': trial_id,\n",
    "            'Last Refreshed On': last_refreshed,\n",
    "            'Public Title': public_title,\n",
    "            'Primary Sponsor': primary_sponsor,\n",
    "            'Web Address' : web_address,\n",
    "            'Recruitment Status' :recruitment_status,\n",
    "            'Countries': countries,\n",
    "            'Contact Firstname': contact_firstname,\n",
    "            'Contact Lastname': contact_lastname,\n",
    "            'Contact Address': contact_address,\n",
    "            'Contact Email': contact_email,\n",
    "            'Contact Tel': contact_tel,\n",
    "            'Affiliation': Contact_Affiliation\n",
    "        })\n",
    "\n",
    "    time.sleep(2)\n",
    "    global df\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df   \n",
    "\n",
    "def process_df(df):\n",
    "    df = df[df['Last Refreshed On'] != '']\n",
    "    df = df[df['Last Refreshed On'] != '']\n",
    "    df['Contact Firstname'] = df['Contact Firstname'].apply(lambda x: str(x).strip())\n",
    "    df['Contact Lastname'] = df['Contact Lastname'].apply(lambda x: str(x).strip())                                                                   \n",
    "    df['Full Name'] = df['Contact Firstname'].astype(str) +' '+ df['Contact Lastname'].astype(str)\n",
    "    df['Full Name'] = df['Full Name'].apply(lambda x: str(x).split(',')[0].replace(';','').replace('nan','').replace('\\n',' ').strip()) \n",
    "    df['Contact Address'] = df['Contact Address'].apply(lambda x: str(x).replace(';','').replace('nan','').strip())\n",
    "    df['Contact Email'] = df['Contact Email'].apply(lambda x: str(x).replace(';','').replace('nan','').strip())\n",
    "    df['Contact Tel'] = df['Contact Tel'].apply(lambda x: str(x).replace(';','').replace('nan','').strip())\n",
    "    df['Affiliation'] = df['Affiliation'].apply(lambda x: str(x).replace(';','').replace('nan','').strip())\n",
    "    df = df.drop(columns=['Contact Firstname', 'Contact Lastname'])\n",
    "    df['Disease Title Searched'] = title   \n",
    "    df['PubMed Total Results'] = ''\n",
    "    df['Google Search Key'] = df['Full Name'].astype(str) + '+' +df['Contact Email'].astype(str)\n",
    "    df['Social Media Links'] = ''\n",
    "    \n",
    "\n",
    "    new_order = ['Source','Disease Title Searched','Data Export Date','Trial ID','Affiliation',\n",
    "             'Full Name', 'Contact Email','Contact Tel','Contact Address',\n",
    "             'Countries','Primary Sponsor','Public Title', 'Last Refreshed On', \n",
    "             'Web Address', 'Recruitment Status', 'PubMed Total Results','Google Search Key','Social Media Links']\n",
    "            \n",
    "            \n",
    "    df = df[new_order]\n",
    "    return df\n",
    "            \n",
    "def run_scraping_pubmed(df):\n",
    "    driver = initialize_driver()\n",
    "    # driver = webdriver.Chrome()\n",
    "    driver.get(pubmedsearch)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        person_name = row['Full Name']\n",
    "        WebDriverWait(driver, 45).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "        WebDriverWait(driver, 45).until(EC.visibility_of_element_located((By.XPATH,'//*[@id=\"id_term\"]')))\n",
    "        search_box = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"id_term\"]')))\n",
    "        search_box.click()\n",
    "        search_box.clear()\n",
    "\n",
    "        time.sleep(1)\n",
    "        search_box = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"id_term\"]')))\n",
    "        driver.execute_script(\"arguments[0].value = '';\", search_box)\n",
    "\n",
    "        search_box = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"id_term\"]')))\n",
    "        search_box.send_keys(person_name)\n",
    "        time.sleep(1)\n",
    "\n",
    "        submit_btn = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"search-form\"]/div/div[1]/div/button')))\n",
    "        submit_btn.click()\n",
    "\n",
    "        WebDriverWait(driver, 30).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"search-results\"]/div[2]/div[1]/div[1]/h3/span')))\n",
    "            last_5_years_filter = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"static-filters-form\"]/div/div[1]/div[1]/ul/li[2]/label')))\n",
    "            last_5_years_filter.click()\n",
    "            time.sleep(1)\n",
    "            total_results = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"search-results\"]/div[2]/div[1]/div[1]/h3/span')))\n",
    "            result_count = total_results.text\n",
    "            \n",
    "        except Exception as e:\n",
    "                try: \n",
    "                    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"article-top-actions-bar\"]/div/div/div[1]/span')))\n",
    "                    result_count = '1'\n",
    "\n",
    "                except Exception as e:\n",
    "                    result_count = 'No Result Found'  \n",
    "\n",
    "        \n",
    "        \n",
    "        # display_option = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"search-form\"]/div[2]/div/div[3]/div[2]/button')))\n",
    "        # display_option.click()\n",
    "        # format = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"id_format\"]')))\n",
    "        # select = Select(format)\n",
    "        # select.select_by_visible_text(\"PubMed\")\n",
    "        df.at[index,'PubMed Total Results'] = result_count\n",
    "\n",
    "    return df\n",
    "        \n",
    "# Function to extract social media links\n",
    "def find_social_media_links(page_source):\n",
    "    social_media_patterns = [\n",
    "        r\"(?i)(facebook\\.com|https://x\\.com|instagram\\.com|linkedin\\.com|tiktok\\.com|pinterest\\.com)\"\n",
    "    ]\n",
    "    \n",
    "    social_links = []\n",
    "    \n",
    "    # Extract all links from the page\n",
    "    links = re.findall(r'href=[\"\\'](https?://[^\\s]+)[\"\\']', page_source)\n",
    "    for link in links:\n",
    "        for pattern in social_media_patterns:\n",
    "            if re.search(pattern, link):\n",
    "                social_links.append(link)\n",
    "                break  # Stop after finding one social media platform in the link\n",
    "\n",
    "    \n",
    "    return social_links\n",
    "\n",
    "# Function to search Google using Selenium\n",
    "\n",
    "def google_search(df):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(google)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait_2 = WebDriverWait(driver,300)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            query = row['Google Search Key']\n",
    "\n",
    "            random_seconds = random.randint(1, 5)  # Random number between 1 and 5 (inclusive)\n",
    "            time.sleep(random_seconds)\n",
    "            \n",
    "            search_box = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"APjFqb\"]')))\n",
    "            search_box.clear()\n",
    "\n",
    "            search_box = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"APjFqb\"]')))\n",
    "            driver.execute_script(\"arguments[0].value = '';\", search_box)\n",
    "\n",
    "            search_box = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"APjFqb\"]')))\n",
    "            search_box.send_keys(query)\n",
    "            search_box.send_keys(Keys.ENTER)  # Press Enter to start search\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 30).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "            if \"www.google.com/sorry/index?\" in driver.current_url:\n",
    "                print(f\"Captcha encountered for query: {query}\")\n",
    "                wait_2.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"APjFqb\"]')))\n",
    "            \n",
    "            random_seconds = random.randint(1, 4)  # Random number between 1 and 4 (inclusive)\n",
    "            time.sleep(random_seconds)\n",
    "    \n",
    "            WebDriverWait(driver, 30).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "            pg_source = driver.page_source\n",
    "            time.sleep(1)\n",
    "\n",
    "            social_media_links = find_social_media_links(pg_source)\n",
    "            df.at[index,'Social Media Links'] = social_media_links\n",
    "        \n",
    "            pg_source = driver.page_source\n",
    "            social_media_links = find_social_media_links(pg_source)\n",
    "            df.at[index, 'Social Media Links'] = social_media_links\n",
    "\n",
    "            time.sleep(2)  # Rate limiting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query}: {e}\")\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_database(df):\n",
    "    df = df.drop(columns=['Google Search Key'])\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    db_name = f\"{title}_results_{timestamp}.xlsx\"\n",
    "    df.to_excel(download_dir_for_excel_files+\"\\\\\"+db_name, index=False)\n",
    "    print(\"File Saved. Please review. Thank you.\")\n",
    "\n",
    "\n",
    "#### Caling all functions\n",
    "\n",
    "print(\"Running Search on WHO....\")\n",
    "run_scraping_who(title, countries)\n",
    "\n",
    "print(\"Reading XML file....\")\n",
    "df = read_latest_xml_file(download_dir_for_xml_files)\n",
    "\n",
    "df = process_df(df)\n",
    "\n",
    "print(\"Running Scrappring on Pubmed....\")\n",
    "df = run_scraping_pubmed(df)\n",
    "\n",
    "print(\"Running Google Search....\")\n",
    "google_search(df)\n",
    "\n",
    "print(\"Saving file....\")\n",
    "save_database(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
